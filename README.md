# ğŸš€ Hackathon Pipeline Airflow

A robust ETL pipeline built with Apache Airflow to scrape, process, and store hackathon data from Devpost and Major League Hacking (MLH) platforms.

## ğŸ¯ Overview

This project implements an automated data pipeline that:
- ğŸ” Scrapes hackathon listings from Devpost's API and MLH's website
- ğŸ”„ Processes and standardizes the data
- â˜ï¸ Stores the results in AWS S3
- â° Runs on a daily schedule using Apache Airflow

## ğŸ—ï¸ Architecture

The pipeline consists of three main components:
1. ğŸŒ Devpost Scraper: Fetches hackathon data from Devpost's API
2. ğŸŒ MLH Scraper: Scrapes hackathon information from MLH's website
3. ğŸ”§ Data Processor: Combines and cleans the data from both sources

## ğŸ“‹ Prerequisites

- ğŸ³ Docker and Docker Compose
- ğŸ Python 3.8+
- â˜ï¸ AWS Account with S3 access
- ğŸŒ¬ï¸ Apache Airflow 2.5.0
- ğŸ› ï¸ Astronomer CLI

## ğŸ“ Project Structure

```
hackathon_pipeline_airflow/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ hackathon_etl_dag.py    # Main Airflow DAG file
â”‚   â””â”€â”€ scrape_devpost.py       # Standalone Devpost scraper
â”‚   â””â”€â”€ scrape_mlh.py           # Standalone MLH scraper
â”œâ”€â”€ docker-compose.yaml          # Docker configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # Project documentation
```

## ğŸš€ Setup and Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd hackathon_pipeline_airflow
```

2. Configure AWS credentials:
   - âœ¨ Set up an AWS S3 bucket
   - ğŸ”‘ Create an Airflow connection for AWS credentials in the Airflow UI:
     - Conn Id: `aws_s3`
     - Conn Type: `Amazon Web Services`
     - Login: AWS access key ID
     - Password: AWS secret access key
     - Extra: `{"region_name": "ap-south-1"}`

3. Start Airflow using Astro CLI:
```bash
astro dev start
```

This command will spin up 4 Docker containers on your machine:
- ğŸ“Š Postgres: Airflow's Metadata Database
- ğŸ–¥ï¸ Webserver: The Airflow component for rendering the Airflow UI
- âš™ï¸ Scheduler: The Airflow component for monitoring and triggering tasks
- ğŸ”„ Triggerer: The Airflow component for triggering deferred tasks

4. Verify the containers:
```bash
docker ps
```

5. Access the Airflow UI:
   - ğŸŒ URL: `http://localhost:8080`
   - ğŸ‘¤ Username: `admin`
   - ğŸ”’ Password: `admin`

Note: If ports 8080 (Webserver) or 5432 (Postgres) are already in use, you can either stop existing Docker containers or change the ports following [Astronomer's documentation](https://www.astronomer.io/docs/astro/cli/troubleshoot-locally#ports-are-not-available-for-my-local-airflow-webserver).

## âš™ï¸ DAG Configuration

The main DAG (`hackathon_etl_dag.py`) runs daily and consists of three tasks:
1. ğŸŒ `scrape_devpost`: Fetches data from Devpost API
2. ğŸŒ `scrape_mlh`: Scrapes data from MLH website
3. ğŸ”„ `combine_and_clean`: Merges and processes data from both sources

## ğŸ’¾ Data Storage

The pipeline stores data in three S3 files:
- ğŸ“ `devpost.csv`: Raw data from Devpost
- ğŸ“ `mlh.csv`: Raw data from MLH
- ğŸ“ `combined.csv`: Processed and cleaned data from both sources

## ğŸ‘©â€ğŸ’» Development

To run the scrapers independently of Airflow:

```bash
# Run Devpost scraper
python scripts/scrape_devpost.py

# Run MLH scraper
python scripts/scrape_mlh.py
```

## ğŸš¨ Error Handling

The pipeline includes comprehensive error handling and logging:
- âŒ Failed API requests are logged and retried
- âœ… Data validation checks are performed at each stage
- ğŸ“§ Task failures trigger email notifications (when configured)

## ğŸ“Š Monitoring

Monitor the pipeline through:
- ğŸ“º Airflow UI dashboard
- ğŸ“ Logging system (configured to INFO level)
- ğŸ“ˆ AWS S3 bucket metrics

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸŒŸ Deploy to Astronomer

If you have an Astronomer account, you can deploy this project to Astronomer. For detailed deployment instructions, refer to [Astronomer's documentation](https://www.astronomer.io/docs/astro/deploy-code/).
